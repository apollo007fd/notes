# Ensamble Learning

集成学习

[TOC]



# Averaging



## Voting Classifiers

采用N个学习算法上, 在全部训练数据集上, 每个学习算法训练一个模型.  如LR, SVM, RF, KNN等.

最后, 可以采用hard voting, 一个分类器一票, 进行投票表决.

也可以采用soft-voting, 将每个分类器分类可信度加权. 这种方式效果一般更好.



## Bagging和Pasting

与voting Classifier不同的是:

1. voting Classifier采用多个学习算法,  Bagging和Pasting只采用一个学习算法.
2. Bagging和Pasting对数据集进行采样, 得到多个数据子集, 而voting Classifier每次都使用全部数据.



### Bagging (boostrap aggregating) [1]

**Boostraping**, 即自助法采样: 它是一种有放回的抽样方法(可能抽到重复的样本).

A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）

B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等） 

C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同） 

Bagging只使用同一种算法, 采用有放回的采样, 采集N组数据集, 并使用同一个算法训练N个模型.



### Pasting

无放回的抽样.



## Boosting [2]

顺序建立一系列弱模型，每一个弱模型都努力降低bias，从而得到一个强模型。例子：AdaBoost，Gradient Boosting。  

其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。 

关于Boosting的两个核心问题：

1）在每一轮如何改变训练数据的权值或概率分布？

通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。

2）通过什么方式来组合弱分类器？

通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。

而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。



## 随机森林 [1]

为了增加个体预测器的多样性，**随机森林**在bagging（决策树）的基础上更进一步，在每棵树的建造过程中增加了随机性。在每棵随机树的生长过程中，节点分裂时，所用的特征不再是所有特征中最好的，而是特征的一个子集中最好的。这种随机性降低了运算量，略微提升了bias，极大提升了决策树的多样性，在取平均数之后可以大幅减小variance。整体而言，随机森林是一个非常强大的算法。 



**用随机森林评估特征的重要性**: 一个特征在决策树上的深度，可以反映它对预测的影响力，也就是它的**重要性**。如果一个特征被用于最顶层的节点分裂，那么它的数值影响了所有的样本，说明它对预测任务非常重要。相比之下，用于最底层节点分裂的特征，只影响了一两个样本，对最终预测的贡献不大。 

​	在随机森林中，我们可以取所有随机树的特征重要性的平均值，得到模型整体的特征重要性。这一数据可以被用于**特征选择和特征提取**。它的一大优点是，特征重要性的计算是在模型训练的过程中自动完成的，不需要额外的计算量（在思路上类似Lasso的自带特征选择）。 















[1]https://zhuanlan.zhihu.com/p/26683576

[2]https://www.cnblogs.com/liuwu265/p/4690486.html
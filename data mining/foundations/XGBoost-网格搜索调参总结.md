# XGBoost-网格搜索调参总结

调参的原则, 是先粗调再微调, 优先调整对模型效果影响比较大的参数...

1.为了提高调参速度, 通常在开始的时候选择一个稍大的learning_rate, 比如0.1, 待调整好其他参数, 再缩小learning_rate, 同时等比例放大n_estimators. 对于不同的问题, 理想的学习速率有时候会在0.05到0.3之间波动.  

2.确定一个学习速率后, 选择对应于此学习速率的理想决策树数量. XGBoost有一个很有用的函数"cv", 这个函数可以在每一次迭代中使用交叉验证, 并返回理想的决策树数量.

3.对于给定的学习速率和决策树数量, 进行决策树特定参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree). 在确定一棵树的过程中, 我们可以选择不同的参数.

4.xgboost的正则化参数的调优. (lambda, alpha). 这些参数可以降低模型的复杂度, 从而提高模型的表现.

5.降低学习速率, 等比例放大数的数量.



以课题的WiFi定位数据集为例, 来演示调参过程.








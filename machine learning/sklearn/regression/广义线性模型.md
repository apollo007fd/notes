# 广义线性模型

## 线性回归LinearRegression

线性回归拟合带有噪音的一维数据：

```
import matplotlib.pyplot as plt
import numpy as np

x = np.arange(-1,1,2.0 / 100).reshape(-1,1)   ## 必须加reshape，不然报错
y = 2 * x  + 0.1 * np.random.randn(100).reshape(-1,1) + 3

regr = linear_model.LinearRegression()
regr.fit(x, y)

a = regr.coef_
b = regr.intercept_    
print(a, b)
y2 = a * x + b

plt.figure()
plt.scatter(x, y)
plt.plot(x, y2)
plt.show()
```

![1529725961400](F:\notes\sklearn\regression\img\1529725961400.png)



总结：

输入的训练数据如果是一维的，shape不可以是(n,)，必须是(n,1)

训练得到的模型，权值向量是regr.coef_ ，常数项是regr.intercept_



## 岭回归Ridge

```
print('------------------ridge regression-------------------')
reg = linear_model.Ridge(alpha=.5)
reg.fit(x,y)
a,b = reg.coef_, reg.intercept_
print(a, b)
y3 = a * x + b
print("Mean squared error: %.2f" % mean_squared_error(x, y3))
plt.figure()
plt.scatter(x, y, color='black')
plt.plot(x,y3,color ='red')
plt.show()
```

总结：Ridge是带有L2正则化的线性回归



### RidgeCV 可以搜索正则化参数的岭回归

```
print('------------------RidgeCV-------------------')
reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])
reg.fit(x, y)
a,b = reg.coef_, reg.intercept_
print(a,b,reg.alpha_)
y4 = a*x + b
print("Mean squared error: %.2f" % mean_squared_error(x, y4))
plt.figure()
plt.scatter(x, y, color='black')
plt.plot(x,y4,color ='red')
plt.show()
```

```
print('--------------------Lasso--------------------')
reg = linear_model.Lasso(alpha=0.1)
reg.fit(x, y)
y4 = reg.predict(x)
print("Mean squared error: %.2f" % mean_squared_error(x, y4))
print(a,b)
plt.figure()
plt.title('Lasso')
plt.scatter(x, y, color='black')
plt.plot(x,y4,color ='red')
plt.show()
```

```
print('------------------LassoLars----------------')
reg = linear_model.LassoLars(alpha=0.1)
reg.fit(x, y)
y4 = reg.predict(x)
print("Mean squared error: %.2f" % mean_squared_error(x, y4))
print(reg.coef_, reg.intercept_)
plt.figure()
plt.title('LassoLars')
plt.scatter(x, y, color='black')
plt.plot(x,y4,color ='red')
plt.show()
```

```
print('------------------BayesianRidge----------------')
reg = linear_model.BayesianRidge()
reg.fit(x, y)
y4 = reg.predict(x)
print("Mean squared error: %.2f" % mean_squared_error(x, y4))
print(reg.coef_, reg.intercept_)
plt.figure()
plt.title('BayesianRidge')
plt.scatter(x, y, color='black')
plt.plot(x,y4,color ='red')
plt.show()
```

out:

------------------linear regression-------------------
[[ 2.02795202]] [ 3.03580261]
Mean squared error: 9.41
------------------Ridge regression-------------------
[[ 1.95460837]] [ 3.03396902]
Mean squared error: 9.36
------------------RidgeCV-------------------
[[ 2.01284623]] [ 3.03542497] 0.1
Mean squared error: 9.40
--------------------Lasso--------------------
Mean squared error: 9.24
[[ 2.01284623]] [ 3.03542497]



## 逻辑回归 Logistic Regression

可以利用带L1正则化项的Logistic Regression进行特征选择，L1正则化会使权值稀疏，相当于进行了特征选择。这个方法可以一试。






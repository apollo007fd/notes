# 神经网络优化算法

## 梯度下降算法

迭代求最小值, 每次都需要计算所有样本的损失;



## 随机梯度下降算法

将训练数据分成batch, 每次只用计算1个batch的损失; 

每次在一个batch上优化神经网络的参数并不会比单个数据慢太多,  而且每次使用一个batch可以大大减少收敛所需的迭代次数, 同时可以使迭代到的结果更加接近梯度下降的效果.



## 反向传播算法





## 牛顿法
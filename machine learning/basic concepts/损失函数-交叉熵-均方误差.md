# 损失函数



## 交叉熵(cross entropy)

给定2个概率分布p和q, 通过q来表示p的交叉熵为:

$H(p, q) = -\sum_x{p(x)}{logq(x)},$

$对于所有的x, p(x) \in [0,1], 且 \sum _x p(X=x)=1 $



交叉熵刻画了2个概率分布之间的距离





### softmax

假设原始的神经网络输出为$y_1, y_2, ... ,y_n$ , 那么经过Softmax回归处理之后的输出为:

$softmax(y_i) = y_{i}^{'} = \frac{e^{yi}}{\sum_{j=1}^{n}{e^{yj}}}$



神经网络的原始输出通过softmax处理后, 满足概率分布的要求: $对于所有的x, p(x) \in [0,1], 且 \sum _x p(X=x)=1 $

p代表的是正确答案, q代表的是预测值.



交叉熵刻画的是2个概率分布的距离, 交叉熵值越小, 2个概率分布越接近.



交叉熵函数不是对称的,$H(p,q) \neq H(q,p)$



$H((1,0,0), (0.5, 0.4, 0.1)) = -(1*log0.5 + 0*log0.4 + 0*log0.1) \simeq 0.3$ 

$H((1,0,0), (0.8, 0.1, 0.1)) = -(1*log0.8 + 0*log0.1 + 0*log0.1) \simeq 0.1$



### tensorflow中交叉熵的实现

```python
cross_entropy = -tf.reduce_mean( y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))
```

tf.clip_by_value(y, 1e-10, 1.0) 将y限制在$[e^{-10}, 1.0]$ 范围内.

"*"在tensorflow中是矩阵点乘,  矩阵乘法要通过tf.matmul实现.



## 均方误差 (MSE, mean squared error)

对于回归问题,最常用的损失函数是均方误差:

$MSE(y, y^{'}) = \frac {\sum^{n}_{i=1}(y_i-y_i^{'})^2} {n}$

tensorflow实现:

```python
mse = tf.reduce_mean(tf.square(y_-y))
```

这里$y_i$ 和y_代表标准答案, y和$y_i^{'}$代表神经网络预测值.



## 自定义损失函数



$Loss(y, y^{'})$ = 

